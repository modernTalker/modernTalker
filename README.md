<!-- 
  âœ¨ Egor Petrov â€” Machine Learning Researcher
  Focus: Theoretical Deep Learning â€¢ Distributed Optimization â€¢ Pipeline Parallelism â€¢ Scaling Laws
-->

<h1 align="center">ğŸ‘‹ Hi, I'm Egor Petrov</h1>
<h3 align="center">ğŸ”¬ Machine Learning Researcher @ <a href="https://research.yandex.com/">Yandex Research</a> | ğŸ“š ICML Tiny Titans / UAI | ğŸ§  LLMs, Optimization & Distributed Systems</h3>

<p align="center">
  <a href="mailto:petrov.egor.d@phystech.edu">
    <img src="https://img.shields.io/badge/Email-petrov.egor.d@phystech.edu-blue?style=flat&logo=gmail" alt="Email"/>
  </a>
  <a href="https://scholar.google.com/citations?user=AlQ_EUEAAAAJ">
    <img src="https://img.shields.io/badge/Google%20Scholar-Profile-green?style=flat&logo=google-scholar" alt="Google Scholar"/>
  </a>
  <a href="https://eng.mipt.ru/">
    <img src="https://img.shields.io/badge/Moscow%20Institute%20of%20Physics%20and%20Technology-Student-purple" alt="MIPT Student"/>
  </a>
</p>

---

## ğŸ” Current Research @ Yandex Research

Iâ€™m actively working on **two core directions**:

1. **Pipeline Parallelism**  
   - Extending **PipeDream 2BW** with novel theoretical improvements  
   - Redesigning scheduling strategies to enhance **asynchronous execution** and reduce pipeline bubbles  
   - Studying **scaling laws** in pipeline-parallel training regimes

2. **Distributed Cluster Learning**  
   - Building on **DiLoco** and **MuLoco** frameworks  
   - Developing communication-efficient, decentralized algorithms for large-scale cluster training  
   - Integrating zeroth-order feedback for robustness in heterogeneous environments

Iâ€™m also finalizing several **unpublished projects** and preparing new submissions to top-tier venues (ICLR, ICML, etc.).

---

## ğŸ§  Research Interests

- **Scaling Laws**: Empirical & theoretical analysis in pipeline and distributed settings; connections to loss landscape geometry (e.g., Hessian spectrum of Transformers)  
- **Pipeline Parallelism**: Scheduling, memory optimization, and convergence guarantees under asynchrony  
- **Theoretical Deep Learning**: Loss landscapes, Bayesian/probabilistic modeling, and applications to **Reinforcement Learning**  
- **Optimization Methods**: Advancing **zeroth-order (ZO)** and **first-order** algorithms for LLMs, decentralized systems, and non-convex problems

---

## ğŸ“š Publications

All peer-reviewed or workshop papers (chronological order):

1. **Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order LLM Fine-Tuning**  
   *E. Petrov, G. Evseev, A. Antonov, A. Veprikov, P. Plyusnin, N. Bushkov et al.*  
   **ICML Workshop on Tiny Titans, 2025**  
   ğŸ“„ [Paper](https://arxiv.org/abs/2506.04430)

2. **When Extragradient Meets PAGE: Bridging Two Giants to Boost Variational Inequalities**  
   *G. Molodtsov, V. Parfenov, E. Petrov, E. Grigoriy, D. Medyakov, A. Beznosikov*  
   **Conference on Uncertainty in Artificial Intelligence (UAI), 2025**  
   ğŸ“„ [Paper](https://openreview.net/forum?id=i9mOhs3UqO)

3. **Zero Order Algorithm for Decentralized Optimization Problems**  
   *A. Veprikov, E. Petrov, G. Evseev, A. Beznosikov*  
   **AI Journey 2024** â€” Published in *Doklady Mathematics (Q2)*  
   ğŸ“„ [Paper](https://link.springer.com/article/10.1134/S1064562424602336)

4. **Shuffling Heuristic in Variational Inequalities: Establishing New Convergence Guarantees**  
   *D. Medyakov, G. Molodtsov, E. Grigoriy, E. Petrov, A. Beznosikov*  
   **ICOMP 2024** â€” Accepted to *JOTA (Q1)*  
   ğŸ† **1st Place, Neuroinformatics 2024**  
   ğŸ“„ [Paper](https://openreview.net/pdf?id=V14SoT0m2q)

5. **Sampling of Semi-Orthogonal Matrices for the Muon Algorithm**  
   *E. D. Petrov, G. V. Evseev, A. V. Antonov, A. S. Veprikov, N. A. Bushkov, S. V. Moiseev, A. N. Beznosikov*  
   Accepted to *Doklady Mathematics (Q2)*  
   ğŸ“„ [Paper]

6. **Full Transformer Analysis: Loss Landscape via Hessian-based Approach**  
   *E. Petrov, N. Kiselev, V. Meshkov, A. Nikitin, A. Grabovoy*  
   Submitted to **ICLR 2026**  
   ğŸ“„ [Paper]

7. **Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning...**  
   *D. Medyakov, S. Stanko, G. Molodtsov, P. Zmushko, G. Evseev, E. Petrov et al.*  
   Submitted to **ICLR 2026**  
   ğŸ“„ [Paper](https://arxiv.org/abs/2506.03725)

---

## ğŸ’» Featured Projects

### ğŸ§ª [ZO-Library]((https://github.com/modernTalker/zero-order-optimization.git)) *(Coming Soon)*
> A **PyTorch-style**, open-source library for **zeroth-order optimization**  
> Implements SignSGD, Muon, and custom ZO methods with unified API  
> Designed for LLM fine-tuning, black-box attacks, and distributed settings

### ğŸ“‰ Transformer Hessian Analysis
> Investigating **loss landscape geometry** of full-scale Transformers using Hessian approximations  
> Links curvature to **scaling laws**, generalization, and optimization dynamics

### âš™ï¸ Production ML @ Yandex (2024â€“2025)
> Built a **CatBoost + Vowpal Wabbit** online feature pipeline â†’ **+0.1% AUC/nDCG**  
> Developed **MapReduce-style data converter** â†’ **20Ã— faster preprocessing**

---

## ğŸ› ï¸ Tech Stack

![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![C++](https://img.shields.io/badge/C%2B%2B-00599C?logo=c%2B%2B&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)
![Hadoop](https://img.shields.io/badge/Apache%20Hadoop-66CCFF?logo=apache&logoColor=black)
![Git](https://img.shields.io/badge/Git-F05032?logo=git&logoColor=white)

---

> ğŸ“ B.S. in Applied Mathematics & Informatics @ [MIPT](https://mipt.ru/english/) (2022â€“2026)
> 
> ğŸ“ Yandex School of Data Analysis @ [YSDA](https://dataschool.yandex.com/) (2025-2026)
> 
> ğŸ¯ Data Science Track (MIPT & Yandex SDA, 2024â€“2026) @ [DS](https://thetahat.ru/) (2023-2026)
> 
> ğŸ§‘â€ğŸ« Teaching Assistant: Algorithms, Data Structures, Intro to AI
